{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\zy\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "from src.train_and_evaluate import *\n",
    "from src.models import *\n",
    "import os\n",
    "import torch.optim\n",
    "from src.expressions_transfer import *\n",
    "import json\n",
    "import random\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def read_json(path):\n",
    "    with open(path,'r') as f:\n",
    "        file = json.load(f)\n",
    "    return file\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Reading lines...\n",
      "{'id': '2', 'original_text': '一个工程队挖土，第一天挖了316方，从第二天开始每天都挖230方，连续挖了6天，这个工程队一周共挖土多少方？', 'segmented_text': '一 个 工程队 挖土 ， 第一天 挖 了 316 方 ， 从 第 二 天 开始 每天 都 挖 230 方 ， 连续 挖 了 6 天 ， 这个 工程队 一周 共 挖土 多少 方 ？', 'equation': 'x=316+230*(6-1)', 'ans': '1466'} 23162\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = load_raw_data(\"data/Math_23K.json\")\n",
    "group_data = read_json(\"data/Math_23K_processed.json\")\n",
    "\n",
    "data = load_raw_data(\"data/Math_23K.json\")\n",
    "\n",
    "print(data[1], len(data))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def remove_non_unicode_chars(text):\n",
    "    all_punc = [\"．\", \"？\", \"（\", \"）\", \",\", \"：\", \"；\", \"？\", \"！\", \"，\", \"“\", \"”\", \",\", \".\", \"?\", \"，\", \"。\", \"？\", \"．\", \"；\", \"｡\", '?', '.','(',')']\n",
    "    split_text = text.split(' ')\n",
    "    cleaned_text = []\n",
    "    for s in split_text:\n",
    "        if s in all_punc:\n",
    "            cleaned_text.append(s)\n",
    "        else:\n",
    "            s = re.sub(r'[^\\x00-\\x7F\\u4E00-\\u9FA5]', '', s)\n",
    "            cleaned_text.append(s)\n",
    "    cleaned = [c for c in cleaned_text if c != '']\n",
    "    return cleaned\n",
    "\n",
    "cleaned_train_data = []\n",
    "for item in data:\n",
    "    cleaned_item = {\n",
    "        'id': item['id'],\n",
    "        'original_text': ''.join(remove_non_unicode_chars(item['segmented_text'])),\n",
    "        'segmented_text': remove_non_unicode_chars(item['segmented_text']),\n",
    "        'equation': item['equation'],\n",
    "        'ans': item['ans']\n",
    "    }\n",
    "    cleaned_train_data.append(cleaned_item)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23162\n"
     ]
    }
   ],
   "source": [
    "print(len(cleaned_train_data))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from ltp import LTP\n",
    "ltp = LTP(path=r\"E:\\research_v2\\tools\\Ltp_base2_v3_\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def transfer_ro_num(data):  # transfer num into \"NUM\"\n",
    "    print(\"Transfer numbers...\")\n",
    "    pattern = re.compile(\"\\d*\\(\\d+/\\d+\\)\\d*|\\d+\\.\\d+%?|\\d+%?\")\n",
    "    pairs = []\n",
    "    generate_nums = []\n",
    "    generate_nums_dict = {}\n",
    "    copy_nums = 0\n",
    "    for d in tqdm.tqdm(data, desc='parse the sentence ... '):\n",
    "        id = d['id']\n",
    "        nums = []\n",
    "        input_seq = []\n",
    "        seg = d[\"segmented_text\"]\n",
    "        ori_seg = []\n",
    "        ans = d['ans']\n",
    "        equations = d['equation'][2:]\n",
    "        for s in seg:\n",
    "            pos = re.search(pattern, s)\n",
    "            if pos and pos.start() == 0:\n",
    "                nums.append(s[pos.start(): pos.end()])\n",
    "                input_seq.append(\"NUM\")\n",
    "                ori_seg.append(s[pos.start(): pos.end()])\n",
    "                if pos.end() < len(s):\n",
    "                    input_seq.append(s[pos.end():])\n",
    "                    ori_seg.append(s[pos.end():])\n",
    "            else:\n",
    "                if len(s) > 0:\n",
    "                    input_seq.append(s)\n",
    "                    ori_seg.append(s)\n",
    "                else:\n",
    "                    continue\n",
    "        if copy_nums < len(nums):\n",
    "            copy_nums = len(nums)\n",
    "\n",
    "        nums_fraction = []\n",
    "        for num in nums:\n",
    "            if re.search(\"\\d*\\(\\d+/\\d+\\)\\d*\", num):\n",
    "                nums_fraction.append(num)\n",
    "        nums_fraction = sorted(nums_fraction, key=lambda x: len(x), reverse=True)\n",
    "\n",
    "        def seg_and_tag(st):  # seg the equation and tag the num\n",
    "            res = []\n",
    "            for n in nums_fraction:\n",
    "                if n in st:\n",
    "                    p_start = st.find(n)\n",
    "                    p_end = p_start + len(n)\n",
    "                    if p_start > 0:\n",
    "                        res += seg_and_tag(st[:p_start])\n",
    "                    if nums.count(n) == 1:\n",
    "                        res.append(\"N\" + str(nums.index(n)))\n",
    "                    else:\n",
    "                        res.append(n)\n",
    "                    if p_end < len(st):\n",
    "                        res += seg_and_tag(st[p_end:])\n",
    "                    return res\n",
    "            pos_st = re.search(\"\\d+\\.\\d+%?|\\d+%?\", st)\n",
    "            if pos_st:\n",
    "                p_start = pos_st.start()\n",
    "                p_end = pos_st.end()\n",
    "                if p_start > 0:\n",
    "                    res += seg_and_tag(st[:p_start])\n",
    "                st_num = st[p_start:p_end]\n",
    "                if nums.count(st_num) == 1:\n",
    "                    res.append(\"N\" + str(nums.index(st_num)))\n",
    "                else:\n",
    "                    res.append(st_num)\n",
    "                if p_end < len(st):\n",
    "                    res += seg_and_tag(st[p_end:])\n",
    "                return res\n",
    "            for ss in st:\n",
    "                res.append(ss)\n",
    "            return res\n",
    "\n",
    "        out_seq = seg_and_tag(equations)\n",
    "        for s in out_seq:  # tag the num which is generated\n",
    "            if s[0].isdigit() and s not in generate_nums and s not in nums:\n",
    "                generate_nums.append(s)\n",
    "                generate_nums_dict[s] = 0\n",
    "            if s in generate_nums and s not in nums:\n",
    "                generate_nums_dict[s] = generate_nums_dict[s] + 1\n",
    "\n",
    "        num_pos = []\n",
    "        for i, j in enumerate(input_seq):\n",
    "            if j == \"NUM\":\n",
    "                num_pos.append(i)\n",
    "        assert len(nums) == len(num_pos)\n",
    "\n",
    "        words, hidden = ltp.seg([ori_seg], is_preseged=True)\n",
    "        dep = ltp.dep(hidden)\n",
    "        parse = [d[1] - 1 for d in dep[0]]\n",
    "\n",
    "        assert len(words[0]) == len(input_seq)\n",
    "        item = {\n",
    "            'id': id,\n",
    "            'original_text': words[0],\n",
    "            'num_text': input_seq,\n",
    "            'infix_equation': out_seq,\n",
    "            'parse': parse,\n",
    "            'nums': nums,\n",
    "            'num_pos': num_pos,\n",
    "            'ans': ans\n",
    "        }\n",
    "        pairs.append(item)\n",
    "\n",
    "    temp_g = []\n",
    "    for g in generate_nums:\n",
    "        if generate_nums_dict[g] >= 5:\n",
    "            temp_g.append(g)\n",
    "\n",
    "    return pairs, temp_g, copy_nums"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载已经处理数据集...\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('./data/train_parse_pairs.json'):\n",
    "    print('加载已经处理数据集...')\n",
    "    with open('./data/train_parse_pairs.json', 'r', encoding='utf-8') as f:\n",
    "        pairs = json.load(f)\n",
    "    generate_nums = ['1', '3.14']\n",
    "    copy_nums = 15\n",
    "else:\n",
    "    print('... ing 处理数据集 ing ...')\n",
    "    pairs, generate_nums, copy_nums = transfer_ro_num(cleaned_train_data)\n",
    "    print(generate_nums, copy_nums)\n",
    "    # val_pairs = transfer_val_num(cleaned_val_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "datas = json.dumps(pairs, ensure_ascii=False, indent=1)\n",
    "with open('./data/train_parse_pairs.json', 'w', encoding='utf-8') as file:\n",
    "    file.write(datas)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "{'id': '23162',\n 'original_text': ['一年级',\n  '和',\n  '二年级',\n  '学生',\n  '到',\n  '小精灵',\n  '剧场',\n  '看',\n  '木偶戏',\n  '，',\n  '一年级',\n  '有',\n  '186',\n  '人',\n  '，',\n  '二年级',\n  '有',\n  '235',\n  '人',\n  '．',\n  '剧院',\n  '共有',\n  '500',\n  '个',\n  '座位',\n  '，',\n  '还有',\n  '多少',\n  '个',\n  '空座位',\n  '？'],\n 'num_text': ['一年级',\n  '和',\n  '二年级',\n  '学生',\n  '到',\n  '小精灵',\n  '剧场',\n  '看',\n  '木偶戏',\n  '，',\n  '一年级',\n  '有',\n  'NUM',\n  '人',\n  '，',\n  '二年级',\n  '有',\n  'NUM',\n  '人',\n  '．',\n  '剧院',\n  '共有',\n  'NUM',\n  '个',\n  '座位',\n  '，',\n  '还有',\n  '多少',\n  '个',\n  '空座位',\n  '？'],\n 'infix_equation': ['N2', '-', 'N0', '-', 'N1'],\n 'parse': [3,\n  2,\n  0,\n  7,\n  7,\n  6,\n  4,\n  -1,\n  7,\n  7,\n  11,\n  7,\n  13,\n  11,\n  11,\n  16,\n  11,\n  18,\n  16,\n  11,\n  21,\n  7,\n  23,\n  24,\n  21,\n  21,\n  21,\n  28,\n  29,\n  26,\n  7],\n 'nums': ['186', '235', '500'],\n 'num_pos': [12, 17, 22],\n 'ans': '79'}"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "{'id': '1', 'group_num': [15, 16, 17, 32, 33, 34, 39, 40, 41]}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_data[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "{'id': '23162', 'group_num': [16, 17, 18, 19, 21, 22, 23, 24, 27, 28, 29, 30]}"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_data[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "temp_pairs = []\n",
    "for p in pairs:\n",
    "    if len(p['num_text']) != len(p['original_text']):\n",
    "        assert 0==1\n",
    "    temp_pairs.append((p['num_text'], from_infix_to_prefix(p['infix_equation']), p['nums'], p['num_pos'], p['parse'],\n",
    "                       p['original_text'], p['id']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['一', '个', '工程队', '挖土', '，', '第一天', '挖', '了', 'NUM', '方', '，', '从', '第', '二', '天', '开始', '每天', '都', '挖', 'NUM', '方', '，', '连续', '挖', '了', 'NUM', '天', '，', '这个', '工程队', '一周', '共', '挖土', '多少', '方', '？'], ['+', 'N0', '*', 'N1', '-', 'N2', '1'], ['316', '230', '6'], [8, 19, 25], [1, 2, 3, -1, 3, 6, 3, 6, 9, 6, 3, 18, 13, 14, 11, 11, 18, 18, 3, 20, 18, 3, 23, 3, 23, 26, 23, 3, 29, 32, 32, 32, 3, 34, 32, 3], ['一', '个', '工程队', '挖土', '，', '第一天', '挖', '了', '316', '方', '，', '从', '第', '二', '天', '开始', '每天', '都', '挖', '230', '方', '，', '连续', '挖', '了', '6', '天', '，', '这个', '工程队', '一周', '共', '挖土', '多少', '方', '？'], '2')\n"
     ]
    }
   ],
   "source": [
    "print(temp_pairs[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "ori_path = './data/'\n",
    "prefix = '23k_processed.json'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def get_train_test_fold(ori_path,prefix,data,pairs,group):\n",
    "    mode_train = 'train'\n",
    "    mode_valid = 'valid'\n",
    "    mode_test = 'test'\n",
    "    train_path = ori_path + mode_train + prefix # data/train23k_processed.json\n",
    "    valid_path = ori_path + mode_valid + prefix\n",
    "    test_path = ori_path + mode_test + prefix\n",
    "    train = read_json(train_path)\n",
    "    train_id = [item['id'] for item in train]\n",
    "    valid = read_json(valid_path)\n",
    "    valid_id = [item['id'] for item in valid]\n",
    "    test = read_json(test_path)\n",
    "    test_id = [item['id'] for item in test]\n",
    "    train_fold = []\n",
    "    valid_fold = []\n",
    "    test_fold = []\n",
    "    for item,pair,g in zip(data, pairs, group):\n",
    "        pair = list(pair)\n",
    "        pair.append(g['group_num'])\n",
    "        pair.append(g['id'])\n",
    "        pair = tuple(pair)\n",
    "        if item['id'] in train_id:\n",
    "            train_fold.append(pair)\n",
    "        elif item['id'] in test_id:\n",
    "            test_fold.append(pair)\n",
    "        else:\n",
    "            valid_fold.append(pair)\n",
    "    return train_fold, test_fold, valid_fold"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "pairs = temp_pairs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "train_fold, test_fold, valid_fold = get_train_test_fold(ori_path, prefix,data, pairs, group_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['一', '个', '工程队', '挖土', '，', '第一天', '挖', '了', 'NUM', '方', '，', '从', '第', '二', '天', '开始', '每天', '都', '挖', 'NUM', '方', '，', '连续', '挖', '了', 'NUM', '天', '，', '这个', '工程队', '一周', '共', '挖土', '多少', '方', '？'], ['+', 'N0', '*', 'N1', '-', 'N2', '1'], ['316', '230', '6'], [8, 19, 25], [1, 2, 3, -1, 3, 6, 3, 6, 9, 6, 3, 18, 13, 14, 11, 11, 18, 18, 3, 20, 18, 3, 23, 3, 23, 26, 23, 3, 29, 32, 32, 32, 3, 34, 32, 3], ['一', '个', '工程队', '挖土', '，', '第一天', '挖', '了', '316', '方', '，', '从', '第', '二', '天', '开始', '每天', '都', '挖', '230', '方', '，', '连续', '挖', '了', '6', '天', '，', '这个', '工程队', '一周', '共', '挖土', '多少', '方', '？'], '2', [6, 7, 8, 15, 16, 17, 21, 22, 23], '2')\n"
     ]
    }
   ],
   "source": [
    "print(train_fold[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['NUM', '/', 'NUM', '的', '商', '，', '加上', 'NUM', '，', '再', '乘', 'NUM', '，', '积', '=', '？'], ['*', '+', '/', 'N1', 'N0', 'N2', 'N3'], ['4', '1.8', '3', '2'], [0, 2, 7, 11], [4, 2, 4, 2, 6, 4, -1, 6, 6, 10, 6, 10, 6, 6, 6, 6], ['4', '/', '1.8', '的', '商', '，', '加上', '3', '，', '再', '乘', '2', '，', '积', '=', '？'], '10941', [0, 1, 1, 2, 3, 6, 7, 8, 10, 11, 12, 12, 13, 14], '10941')\n"
     ]
    }
   ],
   "source": [
    "print(train_fold[10000])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['广场', '新种', '了', '一批', '花木', '，', '其中', 'NUM', '是', '玫瑰', '，', 'NUM', '是', '月季', '．', '已知', '月季', '有', 'NUM', '棵', '，', '玫瑰', '有', '多少', '棵', '？'], ['*', '/', 'N2', 'N1', 'N0'], ['(5/16)', '(3/8)', '36'], [7, 11, 18], [1, 4, 1, 4, -1, 4, 8, 8, 4, 8, 8, 12, 8, 12, 8, 17, 17, 8, 19, 17, 17, 22, 17, 24, 22, 8], ['广场', '新种', '了', '一批', '花木', '，', '其中', '(5/16)', '是', '玫瑰', '，', '(3/8)', '是', '月季', '．', '已知', '月季', '有', '36', '棵', '，', '玫瑰', '有', '多少', '棵', '？'], '23146', [11, 12, 13, 17, 18, 19, 21, 22, 23, 24], '23146')\n"
     ]
    }
   ],
   "source": [
    "print(test_fold[-1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['王', '叔叔', '从', '甲', '城', '到', '乙', '城', '，', '第一天', '行', '了', '全程', '的', 'NUM', '，', '第', '二', '天', '行', '了', '全程', '的', 'NUM', '，', '距', '乙', '城', '还有', 'NUM', '千米', '．', '甲', '乙', '两城', '相距', '多少', '千米', '？'], ['/', 'N2', '-', '-', '1', 'N0', 'N1'], ['40%', '(9/20)', '90'], [14, 23, 29], [1, 10, 5, 4, 2, 10, 7, 5, 5, 10, -1, 10, 14, 12, 10, 10, 17, 18, 19, 10, 19, 19, 21, 21, 10, 28, 27, 25, 10, 30, 28, 10, 34, 32, 35, 10, 37, 35, 10], ['王', '叔叔', '从', '甲', '城', '到', '乙', '城', '，', '第一天', '行', '了', '全程', '的', '40%', '，', '第', '二', '天', '行', '了', '全程', '的', '(9/20)', '，', '距', '乙', '城', '还有', '90', '千米', '．', '甲', '乙', '两城', '相距', '多少', '千米', '？'], '23144', [10, 11, 12, 18, 19, 21, 22, 23, 24, 25, 29, 30, 31, 32], '23144')\n"
     ]
    }
   ],
   "source": [
    "print(valid_fold[-1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at E:\\research_v2\\tools\\have_fine_tune\\chinese_roberta_wwm_ext_D\\No\\model were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at E:\\research_v2\\tools\\have_fine_tune\\chinese_roberta_wwm_ext_D\\No\\model and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert = EncoderChar(bert_path=r\"E:\\research_v2\\tools\\have_fine_tune\\chinese_roberta_wwm_ext_D\\No\\model\", bert_size=768, hidden_size=512, get_word_and_sent=True)\n",
    "start = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "from src.pre_data import prepare_ro_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------  Indexing words...  ----------------------------------------\n",
      " <----------------------------  keep_words 3913 / 10492 = 0.3730  ----------------------------> \n",
      "Indexed 3933 words in input language, 23 words in output\n",
      "Number of training data 21162\n",
      "output_lang.index2word:  ['*', '-', '+', '/', '^', '1', '3.14', 'N0', 'N1', 'N2', 'N3', 'N4', 'N5', 'N6', 'N7', 'N8', 'N9', 'N10', 'N11', 'N12', 'N13', 'N14', 'UNK']\n",
      "Number of testind data 1000\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, train_pairs, test_pairs = prepare_ro_data(train_fold, valid_fold, 5, generate_nums,\n",
    "                                                                   copy_nums, bert.tokenizer, tree=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([57, 58, 59, 60, 48, 61, 62, 45, 24, 63, 48, 64, 65, 66, 67, 68, 69, 44, 62, 24, 63, 48, 70, 62, 45, 24, 67, 48, 71, 59, 72, 73, 60, 56, 63, 74], 36, [2, 7, 0, 8, 1, 9, 5], 7, ['316', '230', '6'], [8, 19, 25], [], [1, 2, 3, -1, 3, 6, 3, 6, 9, 6, 3, 18, 13, 14, 11, 11, 18, 18, 3, 20, 18, 3, 23, 3, 23, 26, 23, 3, 29, 32, 32, 32, 3, 34, 32, 3], [6, 7, 8, 15, 16, 17, 21, 22, 23], ['一', '个', '工程队', '挖土', '，', '第一天', '挖', '了', '316', '方', '，', '从', '第', '二', '天', '开始', '每天', '都', '挖', '230', '方', '，', '连续', '挖', '了', '6', '天', '，', '这个', '工程队', '一周', '共', '挖土', '多少', '方', '？'], array([[0, 1, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 1, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 1, 0]], dtype=int64), [(8, [14]), (19, [27]), (25, [34])])\n"
     ]
    }
   ],
   "source": [
    "print(train_pairs[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}